/**
 * Production-Ready Image Feature Extraction Service
 * Uses ONNX Runtime for reliable, cross-platform image feature extraction
 * No native binding issues - works on Windows, Linux, macOS
 * 
 * This is a permanent solution that doesn't depend on TensorFlow.js
 */

import * as ort from 'onnxruntime-node';
import sharp from 'sharp';
import axios from 'axios';
import { promises as fs } from 'fs';
import path from 'path';

// Cache for the ONNX model
let onnxModel: ort.InferenceSession | null = null;
let modelLoadError: Error | null = null;

/**
 * Download ONNX model if not exists
 * We'll use a lightweight MobileNet ONNX model
 */
async function ensureModelExists(): Promise<string | null> {
  const modelDir = path.join(process.cwd(), 'models');
  const modelPath = path.join(modelDir, 'mobilenet_v2_feature_extractor.onnx');
  
  try {
    // Create models directory if it doesn't exist
    await fs.mkdir(modelDir, { recursive: true });
    
    // Check if model exists
    try {
      await fs.access(modelPath);
      return modelPath;
    } catch {
      // Model doesn't exist - return null (will throw error, no Sharp fallback)
      console.log('ğŸ“¦ ONNX model not found at:', modelPath);
      return null;
    }
  } catch (error) {
    console.warn('âš ï¸ Could not check for ONNX model:', error);
    return null;
  }
}

/**
 * Load ONNX model for feature extraction
 */
async function loadOnnxModel(): Promise<ort.InferenceSession | null> {
  if (onnxModel) {
    return onnxModel;
  }
  
  if (modelLoadError) {
    return null;
  }
  
  try {
    const modelPath = await ensureModelExists();
    if (!modelPath) {
      return null; // Use enhanced Sharp method
    }
    
    console.log('â³ Loading ONNX model for image feature extraction...');
    const startTime = Date.now();
    
    onnxModel = await ort.InferenceSession.create(modelPath, {
      executionProviders: ['cpu'], // Use CPU (works everywhere)
      graphOptimizationLevel: 'all'
    });
    
    const loadTime = Date.now() - startTime;
    console.log(`âœ… ONNX model loaded successfully in ${loadTime}ms`);
    
    return onnxModel;
  } catch (error) {
    modelLoadError = error instanceof Error ? error : new Error(String(error));
    console.warn('âš ï¸ Failed to load ONNX model, using enhanced Sharp method:', modelLoadError.message);
    return null;
  }
}

/**
 * Enhanced Sharp-based feature extraction (Production Ready)
 * This is a permanent solution that works everywhere
 * Extracts comprehensive features: color, texture, shape, histogram
 */
async function extractEnhancedFeatures(imageBuffer: Buffer): Promise<number[]> {
  try {
    const startTime = Date.now();
    
    // Extract multiple feature types for better accuracy
    const features: number[] = [];
    
    // 1. Color Features (RGB histogram + dominant colors) - IMPROVED
    const colorFeatures = await extractColorFeatures(imageBuffer);
    features.push(...colorFeatures);
    
    // 2. Texture Features (gradient, contrast, entropy) - IMPROVED
    const textureFeatures = await extractTextureFeatures(imageBuffer);
    features.push(...textureFeatures);
    
    // 3. Shape Features (edge detection, contours) - IMPROVED
    const shapeFeatures = await extractShapeFeatures(imageBuffer);
    features.push(...shapeFeatures);
    
    // 4. Histogram Features (intensity distribution)
    const histogramFeatures = await extractHistogramFeatures(imageBuffer);
    features.push(...histogramFeatures);
    
    // 5. NEW: Spatial Features - Better for distinguishing objects
    // Extract features from different regions (center, corners) to capture spatial patterns
    const spatialFeatures = await extractSpatialFeatures(imageBuffer);
    features.push(...spatialFeatures);
    
    // 6. NEW: Color Distribution Features - Better color analysis
    const colorDistributionFeatures = await extractColorDistributionFeatures(imageBuffer);
    features.push(...colorDistributionFeatures);
    
    // Normalize to 256 dimensions (good balance)
    while (features.length < 256) {
      features.push(0);
    }
    
    const normalized = normalizeVector(features.slice(0, 256));
    const totalTime = Date.now() - startTime;
    
    console.log(`âœ… Enhanced Sharp feature extraction: ${totalTime}ms, ${normalized.length} dimensions`);
    
    if (totalTime > 500) {
      console.warn(`âš ï¸ Enhanced feature extraction took ${totalTime}ms`);
    }
    
    return normalized;
  } catch (error) {
    console.error('Enhanced feature extraction failed:', error);
    return new Array(256).fill(0);
  }
}

/**
 * Extract color features: RGB histogram, dominant colors, color moments
 */
async function extractColorFeatures(imageBuffer: Buffer): Promise<number[]> {
  const features: number[] = [];
  
  // Get image stats for color analysis
  const stats = await sharp(imageBuffer)
    .resize(64, 64, { fit: 'cover', kernel: sharp.kernel.nearest })
    .stats();
  
  // RGB channel means (3 features)
  if (stats.channels && stats.channels.length >= 3) {
    features.push(
      stats.channels[0].mean / 255, // R
      stats.channels[1].mean / 255, // G
      stats.channels[2].mean / 255  // B
    );
    
    // RGB channel standard deviations (3 features)
    features.push(
      stats.channels[0].stdev / 255,
      stats.channels[1].stdev / 255,
      stats.channels[2].stdev / 255
    );
  } else {
    features.push(0, 0, 0, 0, 0, 0);
  }
  
  // RGB histogram (48 features: 16 bins per channel)
  const rgbHistogram = await extractRGBHistogram(imageBuffer);
  features.push(...rgbHistogram);
  
  return features;
}

/**
 * Extract RGB histogram (16 bins per channel = 48 features)
 */
async function extractRGBHistogram(imageBuffer: Buffer): Promise<number[]> {
  const resized = await sharp(imageBuffer)
    .resize(64, 64, { fit: 'cover', kernel: sharp.kernel.nearest })
    .raw()
    .toBuffer();
  
  const rHist = new Array(16).fill(0);
  const gHist = new Array(16).fill(0);
  const bHist = new Array(16).fill(0);
  
  // Process RGB pixels
  for (let i = 0; i < resized.length; i += 3) {
    if (i + 2 < resized.length) {
      const r = resized[i];
      const g = resized[i + 1];
      const b = resized[i + 2];
      
      rHist[Math.floor((r / 255) * 16)]++;
      gHist[Math.floor((g / 255) * 16)]++;
      bHist[Math.floor((b / 255) * 16)]++;
    }
  }
  
  // Normalize histograms
  const total = resized.length / 3;
  const normalized = [
    ...rHist.map(v => v / total),
    ...gHist.map(v => v / total),
    ...bHist.map(v => v / total)
  ];
  
  return normalized;
}

/**
 * Extract texture features: gradient, contrast, entropy
 */
async function extractTextureFeatures(imageBuffer: Buffer): Promise<number[]> {
  const features: number[] = [];
  
  // Convert to grayscale for texture analysis
  const grayscale = await sharp(imageBuffer)
    .resize(64, 64, { fit: 'cover', kernel: sharp.kernel.nearest })
    .grayscale()
    .raw()
    .toBuffer();
  
  // Calculate gradient (edge strength)
  let gradientSum = 0;
  let contrastSum = 0;
  
  const width = 64;
  const height = 64;
  
  for (let y = 1; y < height - 1; y++) {
    for (let x = 1; x < width - 1; x++) {
      const idx = y * width + x;
      const right = grayscale[idx + 1];
      const bottom = grayscale[idx + width];
      const current = grayscale[idx];
      
      // Gradient magnitude
      const gx = Math.abs(right - current);
      const gy = Math.abs(bottom - current);
      gradientSum += Math.sqrt(gx * gx + gy * gy);
      
      // Contrast (local variance)
      contrastSum += Math.abs(current - ((right + bottom) / 2));
    }
  }
  
  const pixelCount = (width - 2) * (height - 2);
  features.push(
    gradientSum / (pixelCount * 255), // Normalized gradient
    contrastSum / (pixelCount * 255)  // Normalized contrast
  );
  
  // Entropy (information content)
  const histogram = new Array(32).fill(0);
  for (let i = 0; i < grayscale.length; i++) {
    histogram[Math.floor((grayscale[i] / 255) * 32)]++;
  }
  
  let entropy = 0;
  for (let i = 0; i < histogram.length; i++) {
    if (histogram[i] > 0) {
      const p = histogram[i] / grayscale.length;
      entropy -= p * Math.log2(p);
    }
  }
  
  features.push(entropy / 8); // Normalize (max entropy is ~8 for 32 bins)
  
  return features;
}

/**
 * Extract shape features: edge density, aspect ratio
 */
async function extractShapeFeatures(imageBuffer: Buffer): Promise<number[]> {
  const features: number[] = [];
  
  const metadata = await sharp(imageBuffer).metadata();
  const aspectRatio = metadata.width && metadata.height 
    ? metadata.width / metadata.height 
    : 1;
  
  features.push(aspectRatio);
  
  // Edge density (simplified)
  const grayscale = await sharp(imageBuffer)
    .resize(32, 32, { fit: 'cover', kernel: sharp.kernel.nearest })
    .grayscale()
    .raw()
    .toBuffer();
  
  let edgeCount = 0;
  const threshold = 30; // Edge detection threshold
  
  for (let i = 1; i < grayscale.length - 1; i++) {
    const diff = Math.abs(grayscale[i] - grayscale[i + 1]);
    if (diff > threshold) edgeCount++;
  }
  
  features.push(edgeCount / grayscale.length); // Edge density
  
  return features;
}

/**
 * Extract histogram features: intensity distribution
 */
async function extractHistogramFeatures(imageBuffer: Buffer): Promise<number[]> {
  const grayscale = await sharp(imageBuffer)
    .resize(64, 64, { fit: 'cover', kernel: sharp.kernel.nearest })
    .grayscale()
    .raw()
    .toBuffer();
  
  // 32-bin histogram
  const histogram = new Array(32).fill(0);
  for (let i = 0; i < grayscale.length; i++) {
    histogram[Math.floor((grayscale[i] / 255) * 32)]++;
  }
  
  // Normalize
  const total = grayscale.length;
  return histogram.map(v => v / total);
}

/**
 * Normalize vector to unit length
 */
function normalizeVector(features: number[]): number[] {
  const norm = Math.sqrt(features.reduce((sum, val) => sum + val * val, 0));
  if (norm === 0) return features;
  return features.map(v => v / norm);
}

/**
 * Main function: Extract features from image
 * Uses ONNX - NO FALLBACK (Sharp extracts same features for all images)
 */
export async function extractImageFeatures(imageBuffer: Buffer): Promise<number[]> {
  // Try ONNX first
  const model = await loadOnnxModel();
  if (model) {
    try {
      // Preprocess image for ONNX model
      const preprocessed = await preprocessForOnnx(imageBuffer);
      
      // Run inference
      const results = await model.run({ input: preprocessed });
      const output = results.output as ort.Tensor;
      const features = Array.from(output.data as Float32Array);
      
      return normalizeVector(features);
    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : String(error);
      console.error('âŒ ONNX inference failed:', errorMsg);
      throw new Error(`ONNX model inference failed: ${errorMsg}. Cannot proceed without accurate AI model.`);
    }
  }
  
  // NO FALLBACK - Sharp extracts same features for all images (inaccurate)
  throw new Error(
    'No AI model available for image feature extraction.\n' +
    'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n' +
    'ONNX model not found and TensorFlow.js MobileNet failed to load.\n' +
    'Sharp-based extraction is NOT accurate (extracts same features for all images).\n' +
    'This is why you get the same results for different images.\n' +
    '\n' +
    'SOLUTION:\n' +
    '1. Ensure TensorFlow.js MobileNet can load (check internet connection)\n' +
    '2. OR download ONNX model to: models/mobilenet_v2_feature_extractor.onnx\n' +
    '3. Check server logs for MobileNet loading errors\n' +
    'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
  );
}

/**
 * Extract spatial features from different image regions
 * Helps distinguish objects by their spatial distribution
 */
async function extractSpatialFeatures(imageBuffer: Buffer): Promise<number[]> {
  const features: number[] = [];
  
  try {
    const metadata = await sharp(imageBuffer).metadata();
    const width = metadata.width || 100;
    const height = metadata.height || 100;
    
    // Extract features from 4 quadrants + center
    const regions = [
      { left: 0, top: 0, width: Math.floor(width / 2), height: Math.floor(height / 2) }, // Top-left
      { left: Math.floor(width / 2), top: 0, width: Math.floor(width / 2), height: Math.floor(height / 2) }, // Top-right
      { left: 0, top: Math.floor(height / 2), width: Math.floor(width / 2), height: Math.floor(height / 2) }, // Bottom-left
      { left: Math.floor(width / 2), top: Math.floor(height / 2), width: Math.floor(width / 2), height: Math.floor(height / 2) }, // Bottom-right
      { left: Math.floor(width / 4), top: Math.floor(height / 4), width: Math.floor(width / 2), height: Math.floor(height / 2) } // Center
    ];
    
    for (const region of regions) {
      const regionBuffer = await sharp(imageBuffer)
        .extract(region)
        .resize(32, 32, { fit: 'cover' })
        .raw()
        .toBuffer();
      
      // Calculate mean brightness for each region
      let sum = 0;
      for (let i = 0; i < regionBuffer.length; i++) {
        sum += regionBuffer[i];
      }
      features.push(sum / (regionBuffer.length * 255)); // Normalized
    }
  } catch (error) {
    // If spatial extraction fails, add zeros
    features.push(0, 0, 0, 0, 0);
  }
  
  return features;
}

/**
 * Extract color distribution features
 * Better color analysis for distinguishing objects
 */
async function extractColorDistributionFeatures(imageBuffer: Buffer): Promise<number[]> {
  const features: number[] = [];
  
  try {
    const resized = await sharp(imageBuffer)
      .resize(64, 64, { fit: 'cover' })
      .raw()
      .toBuffer();
    
    // Calculate color variance (how spread out colors are)
    let rSum = 0, gSum = 0, bSum = 0;
    let rSumSq = 0, gSumSq = 0, bSumSq = 0;
    const pixelCount = resized.length / 3;
    
    for (let i = 0; i < resized.length; i += 3) {
      const r = resized[i] / 255;
      const g = resized[i + 1] / 255;
      const b = resized[i + 2] / 255;
      
      rSum += r;
      gSum += g;
      bSum += b;
      rSumSq += r * r;
      gSumSq += g * g;
      bSumSq += b * b;
    }
    
    const rMean = rSum / pixelCount;
    const gMean = gSum / pixelCount;
    const bMean = bSum / pixelCount;
    
    const rVar = (rSumSq / pixelCount) - (rMean * rMean);
    const gVar = (gSumSq / pixelCount) - (gMean * gMean);
    const bVar = (bSumSq / pixelCount) - (bMean * bMean);
    
    features.push(rVar, gVar, bVar);
  } catch (error) {
    features.push(0, 0, 0);
  }
  
  return features;
}

/**
 * Preprocess image for ONNX model
 */
async function preprocessForOnnx(imageBuffer: Buffer): Promise<ort.Tensor> {
  // Resize to 224x224 (standard input size)
  const resized = await sharp(imageBuffer)
    .resize(224, 224, { fit: 'cover' })
    .removeAlpha()
    .raw()
    .toBuffer();
  
  // Convert to normalized float32 array [1, 3, 224, 224]
  const normalized = new Float32Array(1 * 3 * 224 * 224);
  
  for (let i = 0; i < 224 * 224; i++) {
    const r = resized[i * 3] / 255.0;
    const g = resized[i * 3 + 1] / 255.0;
    const b = resized[i * 3 + 2] / 255.0;
    
    // Normalize to [-1, 1] range
    normalized[i] = (r - 0.5) / 0.5; // R channel
    normalized[224 * 224 + i] = (g - 0.5) / 0.5; // G channel
    normalized[2 * 224 * 224 + i] = (b - 0.5) / 0.5; // B channel
  }
  
  return new ort.Tensor('float32', normalized, [1, 3, 224, 224]);
}

/**
 * Extract features from image URL
 */
export async function extractFeaturesFromUrl(imageUrl: string): Promise<number[]> {
  try {
    const response = await axios.get(imageUrl, {
      responseType: 'arraybuffer',
      timeout: 8000,
      maxContentLength: 10 * 1024 * 1024,
    });
    const imageBuffer = Buffer.from(response.data);
    return await extractImageFeatures(imageBuffer);
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : String(error);
    throw new Error(`Failed to extract features from image URL: ${errorMessage}`);
  }
}

/**
 * Extract features from image buffer
 */
export async function extractFeaturesFromBuffer(imageBuffer: Buffer): Promise<number[]> {
  return await extractImageFeatures(imageBuffer);
}

export default {
  extractFeaturesFromUrl,
  extractFeaturesFromBuffer,
  extractImageFeatures
};

